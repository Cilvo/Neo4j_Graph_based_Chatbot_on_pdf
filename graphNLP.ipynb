{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import fitz\n",
    "from py2neo import Graph, Node , Relationship\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"file.pdf\"\n",
    "pdf_file = fitz.open(\"file.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j\n",
    "graph = Graph(\"bolt://localhost:7689\", auth=(\"neo4j\", \"password\"))\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a node for the PDF document\n",
    "pdf_node = Node(\"PDF\", name=\"file.pdf\")\n",
    "graph.create(pdf_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Michau/t5-base-en-generate-headline\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Michau/t5-base-en-generate-headline\")\n",
    "\n",
    "def michau_transformer_gen_headlines(article):\n",
    "    encoding = tokenizer.encode_plus(article, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_masks = encoding[\"attention_mask\"]\n",
    "\n",
    "    beam_outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_masks,\n",
    "        max_length= 10,\n",
    "        num_beams=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(beam_outputs[0])\n",
    "    return result.replace(\"<pad>\", \"\").replace(\"</s>\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pg_no in range(len(pdf_file)):\n",
    "    # Get the current page object\n",
    "    page = pdf_file[pg_no]\n",
    "\n",
    "    # Extract the text from the page object\n",
    "    page_text = page.get_text()\n",
    "    # heading = page_text.split('\\n')[0]\n",
    "    heading = michau_transformer_gen_headlines(page_text)\n",
    "\n",
    "\n",
    "    page_text = page_text.split(\"\\n\", 1)\n",
    "    page_text = page_text[1] if len(page_text) > 1 else \"\"\n",
    "\n",
    "    page_text = page_text.replace(\"\\n\", \" \")\n",
    "    page_text = page_text.replace(\"➢\", \" \")\n",
    "\n",
    "    doc = nlp(page_text) \n",
    "\n",
    "    # doc_dict = {}\n",
    "    # for token in doc:\n",
    "    #     doc_dict[token.i] = {\n",
    "    #         'text': token.text,\n",
    "    #         'lemma': token.lemma_,\n",
    "    #         'pos': token.pos_,\n",
    "    #         'tag': token.tag_,\n",
    "    #         'ent': token.ent_type_\n",
    "    #     } \n",
    "\n",
    "    # doc_dict = {\n",
    "    # key.encode(): value\n",
    "    # for key, value in doc_dict.items()\n",
    "    # }\n",
    "\n",
    "    page_node = Node(\"PAGE\", id = pg_no, text = page_text)\n",
    "    pg_rel = Relationship(pdf_node, heading, page_node, data = heading)\n",
    "\n",
    "    graph.create(page_node)\n",
    "    graph.create(pg_rel)\n",
    "    \n",
    "    sent_after_node = None\n",
    "    for sent in doc.sents:\n",
    "        sentence_node = Node(\"Sentence\",topic = heading, text= sent.text)\n",
    "        sent_rel = Relationship(page_node, \"CONTAINS\", sentence_node)\n",
    "    \n",
    "        graph.create(sentence_node)\n",
    "        graph.create(sent_rel) \n",
    "        \n",
    "        if sent_after_node:\n",
    "            sent_after = Relationship(sent_after_node, 'AFTER', sentence_node)\n",
    "            graph.create(sent_after)\n",
    "        sent_after_node = sentence_node  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendance regularization requests   In case of a missed punch, you have an option to regularize your attendance.   Select the date you wish to raise the regularization for.   Actual reason (for missing the punch) to be selected and time needs to be entered using the dropdown time menu.   This request goes to your reporting manager for approval. Post approval your attendance will be regularized.   You can track the status of your “Attendance Regularization Request” under “Attendance Management” menu. PATH :- Home Page -> Attendance -> Attendance Regularization Request \n"
     ]
    }
   ],
   "source": [
    "# Get the current page object\n",
    "page = pdf_file[5]\n",
    "\n",
    "# Extract the text from the page object\n",
    "page_text = page.get_text()\n",
    "page_text = page_text.replace(\"\\n\", \" \")\n",
    "page_text = page_text.replace(\"➢\", \" \")\n",
    "print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(page_text)\n",
    "print(doc)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question:  User can “Create Leave Request” under “Leave” menu. Select the type of leave you wish to apply for. To apply for a half-day leave, click on “First Half” or “Second Half”. For a full-day leave request, choose “Full- Day” On right-hand side, the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval. Post approval, your leave will appear on your calendar. You can track the status in the Leave Menu under “Leave Request” PATH :- Home Page -> Leave -> Create Leave Request\n",
      "Preprocessed Question:  user create leave request leave menu select type leave wish apply apply half day leave click half second half day leave request choose full- day right hand user view holiday list leave balance date fill detail leave period submit request go report manager approval post approval leave appear calendar track status leave menu leave request path home page > leave > create leave request\n",
      "Named Entities:  [('Create Leave Request', 'WORK_OF_ART'), ('a half-day', 'DATE'), ('a full-day', 'DATE'), ('Fill', 'PERSON'), ('the Leave Menu', 'LOC'), ('Leave Request', 'WORK_OF_ART')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the question\n",
    "question = \"User can “Create Leave Request” under “Leave” menu. Select the type of leave you wish to apply for. To apply for a half-day leave, click on “First Half” or “Second Half”. For a full-day leave request, choose “Full- Day” On right-hand side, the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval. Post approval, your leave will appear on your calendar. You can track the status in the Leave Menu under “Leave Request” PATH :- Home Page -> Leave -> Create Leave Request\"\n",
    "\n",
    "# Apply preprocessing steps using Spacy\n",
    "doc = nlp(question)\n",
    "\n",
    "# Tokenization\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Lowercasing\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Stopword Removal\n",
    "filtered_tokens = [token for token in lowercase_tokens if not nlp.vocab[token].is_stop]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(filtered_tokens))]\n",
    "\n",
    "# Punctuation Removal\n",
    "punct_removed_tokens = [token for token in lemmatized_tokens if not nlp.vocab[token].is_punct]\n",
    "\n",
    "# Named Entity Recognition\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "print(\"Original Question: \", question)\n",
    "print(\"Preprocessed Question: \", \" \".join(punct_removed_tokens))\n",
    "print(\"Named Entities: \", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' Attendance Regularization Requests',\n",
       " '3i HR Team - For Any Issue',\n",
       " '3i Infotech Employee & Reporting',\n",
       " 'Attendance Calendar - Real Time Attendance',\n",
       " 'Attendance Regularization Transactions PATH',\n",
       " 'Attendance Regularizations Approval (',\n",
       " 'HonoHR Application URL: https://3',\n",
       " 'Leave Balance PATH :- Home Page',\n",
       " 'Leave Request - How to Apply for',\n",
       " 'Leave Transactions PATH :- Home',\n",
       " 'Leave and Attendance for Employee & Report',\n",
       " 'Out on Duty Request - How to Track',\n",
       " 'Out on Duty Transactions (OD)',\n",
       " 'Out on Duty Transactions PATH :',\n",
       " 'Reporting Manager Create Employee’s Roster',\n",
       " 'Reporting Manager Create Leave Request on behalf of',\n",
       " 'Reporting Manager PATH :- Home',\n",
       " 'Reporting Manager To Approve Leave Request',\n",
       " 'Reporting Manager Upload Shift Roster',\n",
       " 'Reporting Manager View Employee’s Leave Details',\n",
       " 'Shift Change Request Approval',\n",
       " 'Shift Roster - View Shi'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "# Create a Graph object and connect to the database\n",
    "graph = Graph(\"bolt://localhost:7689\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "\n",
    "# Define a Cypher query to get all nodes with the given label and property value\n",
    "query = f\"MATCH (n)-[r]->(m) WHERE n.name = 'file.pdf' RETURN r.data as heading\"\n",
    "\n",
    "# Run the query and print the results\n",
    "results = graph.run(query)\n",
    "df = pd.DataFrame(results.data())\n",
    "\n",
    "topics = set(df.heading)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionUnavailable",
     "evalue": "Cannot open connection to ConnectionProfile('bolt://localhost:7687')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:806\u001b[0m, in \u001b[0;36mConnectionPool.acquire\u001b[1;34m(self, force_reset, can_overfill)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m     \u001b[39m# Plan A: select a free connection from the pool\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m     cx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_free_list\u001b[39m.\u001b[39;49mpopleft()\n\u001b[0;32m    807\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\wiring.py:62\u001b[0m, in \u001b[0;36mWire.open\u001b[1;34m(cls, address, timeout, keep_alive, on_broken)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     s\u001b[39m.\u001b[39;49mconnect(address)\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIOError\u001b[39;00m, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m error:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mWireError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\bolt.py:355\u001b[0m, in \u001b[0;36mBolt.open\u001b[1;34m(cls, profile, user_agent, on_release, on_broken)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m     wire \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(profile, on_broken\u001b[39m=\u001b[39;49mon_broken)\n\u001b[0;32m    356\u001b[0m     protocol_version \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_handshake(wire)\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\bolt.py:369\u001b[0m, in \u001b[0;36mBolt._connect\u001b[1;34m(cls, profile, on_broken)\u001b[0m\n\u001b[0;32m    368\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m[#\u001b[39m\u001b[39m%04X\u001b[39;00m\u001b[39m] C: (Dialing <\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m>)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m, profile\u001b[39m.\u001b[39maddress)\n\u001b[1;32m--> 369\u001b[0m wire \u001b[39m=\u001b[39m Wire\u001b[39m.\u001b[39;49mopen(profile\u001b[39m.\u001b[39;49maddress, keep_alive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, on_broken\u001b[39m=\u001b[39;49mon_broken)\n\u001b[0;32m    370\u001b[0m local_port \u001b[39m=\u001b[39m wire\u001b[39m.\u001b[39mlocal_address\u001b[39m.\u001b[39mport_number\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\wiring.py:64\u001b[0m, in \u001b[0;36mWire.open\u001b[1;34m(cls, address, timeout, keep_alive, on_broken)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIOError\u001b[39;00m, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m---> 64\u001b[0m     raise_from(WireError(\u001b[39m\"\u001b[39;49m\u001b[39mCannot connect to \u001b[39;49m\u001b[39m%r\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m (address,)), error)\n\u001b[0;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(s, on_broken\u001b[39m=\u001b[39mon_broken)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mWireError\u001b[0m: Cannot connect to IPv4Address(('localhost', 7687))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectionUnavailable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 12\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create a Graph object and connect to the database\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m graph \u001b[39m=\u001b[39m Graph(\u001b[39m\"\u001b[39;49m\u001b[39mbolt://localhost:7687\u001b[39;49m\u001b[39m\"\u001b[39;49m, auth\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mneo4j\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpassword\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Define a Cypher query to get all nodes with the given label and property value\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMATCH (n)-[r]->(m) WHERE n.name = \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfile.pdf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m RETURN r.data AS heading\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\database.py:288\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[1;34m(self, profile, name, **settings)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, profile\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msettings):\n\u001b[1;32m--> 288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m GraphService(profile, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msettings)\n\u001b[0;32m    289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m=\u001b[39m name\n\u001b[0;32m    290\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema \u001b[39m=\u001b[39m Schema(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\database.py:119\u001b[0m, in \u001b[0;36mGraphService.__init__\u001b[1;34m(self, profile, **settings)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m connector_settings[\u001b[39m\"\u001b[39m\u001b[39minit_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m profile\u001b[39m.\u001b[39mrouting:\n\u001b[0;32m    117\u001b[0m     \u001b[39m# Ensures credentials are checked on construction\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     connector_settings[\u001b[39m\"\u001b[39m\u001b[39minit_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connector \u001b[39m=\u001b[39m Connector(profile, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconnector_settings)\n\u001b[0;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graphs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:960\u001b[0m, in \u001b[0;36mConnector.__init__\u001b[1;34m(self, profile, user_agent, init_size, max_size, max_age, routing_refresh_ttl)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_router \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 960\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_pools(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initial_routers)\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:982\u001b[0m, in \u001b[0;36mConnector._add_pools\u001b[1;34m(self, *profiles)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    981\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAdding connection pool for profile \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, profile)\n\u001b[1;32m--> 982\u001b[0m pool \u001b[39m=\u001b[39m ConnectionPool\u001b[39m.\u001b[39;49mopen(\n\u001b[0;32m    983\u001b[0m     profile,\n\u001b[0;32m    984\u001b[0m     user_agent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_agent,\n\u001b[0;32m    985\u001b[0m     init_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_size,\n\u001b[0;32m    986\u001b[0m     max_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_size,\n\u001b[0;32m    987\u001b[0m     max_age\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_age,\n\u001b[0;32m    988\u001b[0m     on_broken\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_broken)\n\u001b[0;32m    989\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pools[profile] \u001b[39m=\u001b[39m pool\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:649\u001b[0m, in \u001b[0;36mConnectionPool.open\u001b[1;34m(cls, profile, user_agent, init_size, max_size, max_age, on_broken)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39m\"\"\" Create a new connection pool, with an option to seed one\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39mor more initial connections.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39m    scheme\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    648\u001b[0m pool \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(profile, user_agent, max_size, max_age, on_broken)\n\u001b[1;32m--> 649\u001b[0m seeds \u001b[39m=\u001b[39m [pool\u001b[39m.\u001b[39macquire() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(init_size \u001b[39mor\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdefault_init_size)]\n\u001b[0;32m    650\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds:\n\u001b[0;32m    651\u001b[0m     seed\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:649\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39m\"\"\" Create a new connection pool, with an option to seed one\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39mor more initial connections.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39m    scheme\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    648\u001b[0m pool \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(profile, user_agent, max_size, max_age, on_broken)\n\u001b[1;32m--> 649\u001b[0m seeds \u001b[39m=\u001b[39m [pool\u001b[39m.\u001b[39;49macquire() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(init_size \u001b[39mor\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdefault_init_size)]\n\u001b[0;32m    650\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds:\n\u001b[0;32m    651\u001b[0m     seed\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:813\u001b[0m, in \u001b[0;36mConnectionPool.acquire\u001b[1;34m(self, force_reset, can_overfill)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    808\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_capacity() \u001b[39mor\u001b[39;00m can_overfill:\n\u001b[0;32m    809\u001b[0m         \u001b[39m# Plan B: if the pool isn't full, open\u001b[39;00m\n\u001b[0;32m    810\u001b[0m         \u001b[39m# a new connection. This may raise a\u001b[39;00m\n\u001b[0;32m    811\u001b[0m         \u001b[39m# ConnectionUnavailable exception, which\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[39m# should bubble up to the caller.\u001b[39;00m\n\u001b[1;32m--> 813\u001b[0m         cx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect()\n\u001b[0;32m    814\u001b[0m         \u001b[39mif\u001b[39;00m cx\u001b[39m.\u001b[39msupports_multi():\n\u001b[0;32m    815\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_supports_multi \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:764\u001b[0m, in \u001b[0;36mConnectionPool._connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_connect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    762\u001b[0m     \u001b[39m\"\"\" Open and return a new connection.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     cx \u001b[39m=\u001b[39m Connection\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprofile, user_agent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_agent,\n\u001b[0;32m    765\u001b[0m                          on_release\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m c: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelease(c),\n\u001b[0;32m    766\u001b[0m                          on_broken\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m msg: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__on_broken(msg))\n\u001b[0;32m    767\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_server_agent \u001b[39m=\u001b[39m cx\u001b[39m.\u001b[39mserver_agent\n\u001b[0;32m    768\u001b[0m     \u001b[39mreturn\u001b[39;00m cx\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\__init__.py:174\u001b[0m, in \u001b[0;36mConnection.open\u001b[1;34m(cls, profile, user_agent, on_release, on_broken)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m profile\u001b[39m.\u001b[39mprotocol \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbolt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpy2neo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbolt\u001b[39;00m \u001b[39mimport\u001b[39;00m Bolt\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m Bolt\u001b[39m.\u001b[39;49mopen(profile, user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    175\u001b[0m                      on_release\u001b[39m=\u001b[39;49mon_release, on_broken\u001b[39m=\u001b[39;49mon_broken)\n\u001b[0;32m    176\u001b[0m \u001b[39melif\u001b[39;00m profile\u001b[39m.\u001b[39mprotocol \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpy2neo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhttp\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTP\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py2neo\\client\\bolt.py:364\u001b[0m, in \u001b[0;36mBolt.open\u001b[1;34m(cls, profile, user_agent, on_release, on_broken)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m bolt\n\u001b[0;32m    363\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, WireError) \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m--> 364\u001b[0m     raise_from(ConnectionUnavailable(\u001b[39m\"\u001b[39;49m\u001b[39mCannot open connection to \u001b[39;49m\u001b[39m%r\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m profile), error)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mConnectionUnavailable\u001b[0m: Cannot open connection to ConnectionProfile('bolt://localhost:7687')"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from py2neo import Graph\n",
    "import streamlit as st\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Create a Graph object and connect to the database\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Define a Cypher query to get all nodes with the given label and property value\n",
    "query = f\"MATCH (n)-[r]->(m) WHERE n.name = 'file.pdf' RETURN r.data AS heading\"\n",
    "\n",
    "# Run the query and extract information\n",
    "results = graph.run(query)\n",
    "df = pd.DataFrame(results.data())\n",
    "topics = set(df.heading)\n",
    "\n",
    "def process(query):\n",
    "    doc_query = nlp(query)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = [token.text for token in doc_query]\n",
    "\n",
    "    # Lowercasing\n",
    "    lowercase_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Stopword Removal\n",
    "    filtered_tokens = [token for token in lowercase_tokens if not nlp.vocab[token].is_stop]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(filtered_tokens))]\n",
    "\n",
    "    # Punctuation Removal\n",
    "    punct_removed_tokens = [token for token in lemmatized_tokens if not nlp.vocab[token].is_punct]\n",
    "\n",
    "    query_processed =  \" \".join(punct_removed_tokens)\n",
    "\n",
    "    return query_processed\n",
    "\n",
    "\n",
    "def query_sim(query_processed):\n",
    "    doc = nlp(query_processed)\n",
    "    sim_list = []\n",
    "    for topic in topics:\n",
    "        rel = nlp(topic)\n",
    "        sim = doc.similarity(rel)\n",
    "        sim_list.append(sim)\n",
    "    max_sim = max(sim_list)\n",
    "    max_index = sim_list.index(max_sim)\n",
    "    sim_topic = topic[max_index]\n",
    "\n",
    "    return sim_topic\n",
    "\n",
    "def answer(sim_topic):\n",
    "    # Define a Cypher query to get all nodes with the given label and property value\n",
    "    answer_query = f\"MATCH (n) WHERE r.data = {sim_topic} RETURN r.data AS heading\"\n",
    "\n",
    "    # Run the query and extract information\n",
    "    results = graph.run(query)\n",
    "    df = pd.DataFrame(results.data())\n",
    "    answer = set(df.heading)\n",
    "    st.write(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  Attendance regularization requests   In case of a missed punch, you have an option to regularize your attendance.   Select the date you wish to raise the regularization for.   Actual reason (for missing the punch) to be selected and time needs to be entered using the dropdown time menu.   This request goes to your reporting manager for approval. Post approval your attendance will be regularized.   You can track the status of your “Attendance Regularization Request” under “Attendance Management” menu. PATH :- Home Page -> Attendance -> Attendance Regularization Request \n",
      "Generated summary:  In case of a missed punch, you have an option to regularize your attendance. Select the date you wish to raise the regularization for. Actual reason (for missing the punch) to be selected and time needs to be entered. This request goes to your reporting manager for approval. Post approval your attendance will be regularized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Set the input text\n",
    "text = page_text\n",
    "\n",
    "# Encode the input text\n",
    "inputs = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(inputs, num_beams=4, max_length=70, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input text: \", text)\n",
    "print(\"Generated summary: \", summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      "approval 0.0\n",
      "approve 0.39349963397048116\n",
      "approved 0.0\n",
      "as 0.14963330713297865\n",
      "attendance 0.0\n",
      "can 0.19674981698524058\n",
      "day 0.0\n",
      "duty 0.2992666142659573\n",
      "for 0.0\n",
      "from 0.19674981698524058\n",
      "manager 0.19674981698524058\n",
      "od 0.2992666142659573\n",
      "on 0.2992666142659573\n",
      "once 0.0\n",
      "or 0.19674981698524058\n",
      "out 0.2992666142659573\n",
      "page 0.19674981698524058\n",
      "path 0.0\n",
      "present 0.0\n",
      "reflect 0.0\n",
      "reject 0.19674981698524058\n",
      "request 0.0\n",
      "requests 0.19674981698524058\n",
      "status 0.0\n",
      "text 0.19674981698524058\n",
      "that 0.0\n",
      "the 0.0\n",
      "this 0.19674981698524058\n",
      "to 0.19674981698524058\n",
      "transactions 0.19674981698524058\n",
      "will 0.0\n",
      "you 0.19674981698524058\n",
      "\n",
      "Document 1\n",
      "approval 0.0\n",
      "approve 0.0\n",
      "approved 0.30746098821535434\n",
      "as 0.2338320064840948\n",
      "attendance 0.0\n",
      "can 0.0\n",
      "day 0.30746098821535434\n",
      "duty 0.0\n",
      "for 0.30746098821535434\n",
      "from 0.0\n",
      "manager 0.0\n",
      "od 0.0\n",
      "on 0.0\n",
      "once 0.30746098821535434\n",
      "or 0.0\n",
      "out 0.0\n",
      "page 0.0\n",
      "path 0.0\n",
      "present 0.30746098821535434\n",
      "reflect 0.30746098821535434\n",
      "reject 0.0\n",
      "request 0.0\n",
      "requests 0.0\n",
      "status 0.30746098821535434\n",
      "text 0.0\n",
      "that 0.30746098821535434\n",
      "the 0.30746098821535434\n",
      "this 0.0\n",
      "to 0.0\n",
      "transactions 0.0\n",
      "will 0.30746098821535434\n",
      "you 0.0\n",
      "\n",
      "Document 2\n",
      "approval 0.3979802707840827\n",
      "approve 0.0\n",
      "approved 0.0\n",
      "as 0.0\n",
      "attendance 0.3979802707840827\n",
      "can 0.0\n",
      "day 0.0\n",
      "duty 0.30267425405314585\n",
      "for 0.0\n",
      "from 0.0\n",
      "manager 0.0\n",
      "od 0.30267425405314585\n",
      "on 0.30267425405314585\n",
      "once 0.0\n",
      "or 0.0\n",
      "out 0.30267425405314585\n",
      "page 0.0\n",
      "path 0.3979802707840827\n",
      "present 0.0\n",
      "reflect 0.0\n",
      "reject 0.0\n",
      "request 0.3979802707840827\n",
      "requests 0.0\n",
      "status 0.0\n",
      "text 0.0\n",
      "that 0.0\n",
      "the 0.0\n",
      "this 0.0\n",
      "to 0.0\n",
      "transactions 0.0\n",
      "will 0.0\n",
      "you 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the list of documents\n",
    "documents = sentence\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "# Get the TF-IDF matrix for the documents\n",
    "tfidf_matrix = vectorizer.transform(documents)\n",
    "\n",
    "# Get the feature names (i.e. the words) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF matrix for each document\n",
    "for i, document in enumerate(documents):\n",
    "    print(\"Document\", i)\n",
    "    for j, feature in enumerate(feature_names):\n",
    "        print(feature, tfidf_matrix[i, j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('text: To approve Out on Duty Transactions  (OD) :-   As  a Manager you can approve or  reject Out on Duty (OD) requests  from this page.   Once approved, the status will  reflect as “Present” for that day. PATH :- Attendance -> Out On Duty Request Approval (OD) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subject matter of the paragraph is: \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# define the paragraph to analyze\n",
    "paragraph = 'To approve Out on Duty Transactions  (OD) :-   As  a Manager you can approve or  reject Out on Duty (OD) requests  from this page.   Once approved, the status will  reflect as “Present” for that day. PATH :- Attendance -> Out On Duty Request Approval (OD) '\n",
    "\n",
    "# create a spaCy document object\n",
    "doc = nlp(paragraph)\n",
    "\n",
    "# initialize a list to store identified entities\n",
    "entities = []\n",
    "\n",
    "# loop through each entity in the document\n",
    "for ent in doc.ents:\n",
    "    # if the entity is a noun or proper noun, add it to the list\n",
    "    if ent.label_ == 'NOUN' or ent.label_ == 'PROPN':\n",
    "        entities.append(ent.text)\n",
    "\n",
    "# join the entities into a string and print it as the subject matter\n",
    "subject = ', '.join(entities)\n",
    "print(\"The subject matter of the paragraph is:\", subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.8350499057520981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample strings\n",
    "string1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "string2 = \"The quick brown fox jumps over the lazy cat.\"\n",
    "\n",
    "# Create TF-IDF vectors for the strings\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform([string1, string2])\n",
    "\n",
    "# Calculate cosine similarity between the vectors\n",
    "cosine_sim = cosine_similarity(tfidf[0], tfidf[1])[0][0]\n",
    "\n",
    "print(\"Cosine similarity:\", cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "def simplet5_transformer_gen_headlines(input, samples=3):\n",
    "    \"\"\"\n",
    "    This function will generate the given number of one-line summaries for the given text input.\n",
    "    \n",
    "    Args:\n",
    "    input (str): Text to summarize\n",
    "    samples (int): Number of samples for one-line summary, default set to 3\n",
    "    \n",
    "    Returns:\n",
    "    preds (list): List of generated summaries\n",
    "    \"\"\"\n",
    "    model_name = \"snrspeaks/t5-one-line-summary\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    input_ids = tokenizer.encode(input, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    generated_ids = model.generate(input_ids=input_ids,\n",
    "                                    num_beams=5,\n",
    "                                    max_length=50,\n",
    "                                    repetition_penalty=2.5,\n",
    "                                    length_penalty=1,\n",
    "                                    early_stopping=True,\n",
    "                                    num_return_sequences=samples)\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPTJConfig, IBertConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LongformerConfig, LukeConfig, LxmertConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OPTConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SplinterConfig, SqueezeBertConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 24\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForQuestionAnswering\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Define the input question and context\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is the capital of France?\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:474\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[1;32m--> 474\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPTJConfig, IBertConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LongformerConfig, LukeConfig, LxmertConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OPTConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SplinterConfig, SqueezeBertConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define the input question and context\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country located in Western Europe. Its capital is Paris.\"\n",
    "\n",
    "# Encode the input question and context as input IDs and attention masks\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Use the model to generate an answer to the question\n",
    "start_scores, end_scores = model(input_ids, attention_mask=attention_mask, output_attentions=False).values()\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores)\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index+1]))\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the answer to the user's query.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def print_letter_by_letter(text, delay=0.03):\n",
    "    for char in text:\n",
    "        print(char, end='', flush=True)\n",
    "        time.sleep(delay)\n",
    "    print()\n",
    "\n",
    "# Example usage\n",
    "answer = \"This is the answer to the user's query.\"\n",
    "print_letter_by_letter(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/feed/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Define the pattern to match special characters (excluding URLs)\n",
    "    pattern = r'[^a-zA-Z0-9\\s\\/:.-]'\n",
    "    \n",
    "    # Remove special characters using regular expressions\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "text_with_special_chars = \"https://www.linkedin.com/feed/\"\n",
    "cleaned_text = remove_special_characters(text_with_special_chars)\n",
    "print(cleaned_text)  # Output: Hello How are you doing Excited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "# Connect to the Neo4j database\n",
    "uri = \"bolt://localhost:7689\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "graph = Graph(uri, auth=(username, password))\n",
    "\n",
    "# Define the Cypher query\n",
    "cypher_query = \"MATCH (pdf:PDF) RETURN pdf.name\"\n",
    "\n",
    "# Execute the query and retrieve the data property values\n",
    "try:\n",
    "\n",
    "    data_list = graph.run(cypher_query).to_table()[0]\n",
    "\n",
    "except:\n",
    "    data_list = []\n",
    "\n",
    "# Print the list of data property values\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(98.8799819946289, 87.33995819091797, 358.5650634765625, 121.94580078125, ' \\nDate :                                   \\nLocation :  \\n \\n', 0, 0), (98.87995910644531, 122.49993133544922, 385.5649108886719, 146.66571044921875, 'Employee Name :  \\n \\nClaimant Name : \\n \\n', 1, 0), (98.87995910644531, 147.2198944091797, 393.0048828125, 171.3857421875, 'Claim intimation no :  \\n \\nCorporate Name :  \\n', 2, 0), (98.87995910644531, 171.8198699951172, 237.56546020507812, 208.2257080078125, 'Type of Claim (Main claim/Pre-\\nPost/Deduction/Deficiency) \\n \\n', 3, 0), (310.79986572265625, 171.8198699951172, 416.5250244140625, 183.9857177734375, 'No of Pages submitted : \\n', 4, 0), (305.3998718261719, 208.7403564453125, 376.5857849121094, 224.21226501464844, 'CHECK LIST \\n', 5, 0), (98.87995910644531, 224.49986267089844, 499.20489501953125, 248.90570068359375, 'Sr. no \\nParticulars  \\nCollected  \\nYes/No \\n', 6, 0), (98.87995910644531, 249.33982849121094, 456.2450256347656, 262.666259765625, '1 \\nDuly Filled & signed Claim Form of IRDA. \\n \\n', 7, 0), (98.87995910644531, 263.00201416015625, 440.48382568359375, 301.585693359375, '2 \\nOriginal Discharge Card / Summary/Transfer Summary/Death \\nSummary \\n \\n', 8, 0), (453.9598083496094, 263.0198974609375, 456.2450256347656, 275.18572998046875, ' \\n', 9, 0), (98.87995910644531, 302.0198669433594, 456.24505615234375, 315.34625244140625, '3 \\nOriginal Final Bill of the Hospital with breakup of all charges  \\n \\n', 10, 0), (98.87995910644531, 315.6819763183594, 442.40380859375, 354.3856506347656, '4 \\nOriginal Bill Paid Receipt (Deposit/Final payment receipt) with \\nrevenue stamp \\n \\n', 11, 0), (453.9598388671875, 315.6998291015625, 456.24505615234375, 327.86566162109375, ' \\n', 12, 0), (98.87995910644531, 354.81982421875, 456.24505615234375, 381.3462219238281, '5 \\nOriginal Investigation Reports (ECG, USG, CT Scan, X-ray, Blood \\nreport, A scan etc) \\n \\n', 13, 0), (98.87995910644531, 381.68194580078125, 456.2450256347656, 394.90618896484375, '6 \\nAll Imaging Films, ECG Strips, Doppler / Angiogram CD etc \\n \\n', 14, 0), (98.87995910644531, 395.3797912597656, 456.2450256347656, 408.7062072753906, '7 \\nOriginal Pharmacy bill with supporting prescriptions.  \\n \\n', 15, 0), (98.87995910644531, 409.059814453125, 429.80377197265625, 447.74560546875, '8 \\nHospital Registration Certificate (in case of a unknown small \\nhospital) \\n \\n', 16, 0), (453.9598388671875, 409.0597839355469, 456.24505615234375, 421.2256164550781, ' \\n', 17, 0), (98.87995910644531, 448.059814453125, 380.4050598144531, 473.42559814453125, '9 \\nAny other original documents related to the claim. \\n \\n', 18, 0), (453.9598083496094, 448.059814453125, 456.2450256347656, 460.22564697265625, ' \\n', 19, 0), (98.87995910644531, 473.9619140625, 456.24505615234375, 499.3455810546875, '10 \\nMLC/FIR in case of Accident cases. \\n \\n \\n', 20, 0), (98.87995910644531, 499.7619323730469, 456.2450256347656, 525.1455688476562, '11 \\nCopy of intimation mail  \\n \\n \\n', 21, 0), (98.87995910644531, 525.5797729492188, 456.24505615234375, 538.9061279296875, '12 \\nContact details of insured  & patient is mandatory \\n \\n', 22, 0), (98.87995910644531, 539.2418823242188, 456.2450256347656, 564.6255493164062, '13 \\nPhoto ID proof of patient is mandatory.  \\n \\n \\n', 23, 0), (98.87995910644531, 565.0597534179688, 456.2450256347656, 590.4255981445312, '14 \\nClarification letter if delay in submission of claim documents. \\n \\n \\n', 24, 0), (98.87995910644531, 590.8597412109375, 420.80389404296875, 617.3861083984375, '15 \\nKYC of patient and Insured, contact details of Claimant and \\nemail \\n', 25, 0), (453.9598083496094, 590.8597412109375, 456.2450256347656, 603.0255737304688, ' \\n', 26, 0), (98.87995910644531, 617.8596801757812, 456.24505615234375, 631.1860961914062, '16 \\nIndoor case papers is mandatory. \\n \\n', 27, 0), (98.87995910644531, 631.5218505859375, 437.9628601074219, 670.1055297851562, '17 \\nAadhar Card of patient is mandatory. as per IRDA Circular \\n# IRDA/SDD/MISC/CIR/248/11/2017 \\n \\n', 28, 0), (453.9598083496094, 631.5397338867188, 456.2450256347656, 643.70556640625, ' \\n', 29, 0), (98.87995910644531, 670.5396728515625, 456.2425231933594, 682.7055053710938, '18 \\nOriginal Cancel cheque with name of beneficiary & IFSC code \\n \\n', 30, 0), (93.59996032714844, 683.1397094726562, 460.4449462890625, 732.1455078125, ' \\nName & Signature of Employee :  \\nName & signature of the receiver  \\n \\n \\n', 31, 0), (93.59996032714844, 732.5796508789062, 95.88516998291016, 744.7454833984375, ' \\n', 32, 0), (93.5999984741211, 67.68000030517578, 272.7599792480469, 97.44000244140625, '<image: DeviceRGB, width: 148, height: 42, bpc: 8>', 33, 1)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 26\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Specify the page number containing the table\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m page_number \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m table_text \u001b[39m=\u001b[39m extract_table_text(pdf_path, page_number)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(table_text)\n",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 26\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m table_text \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m table_blocks:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m block\u001b[39m.\u001b[39;49mrows:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         table_text\u001b[39m.\u001b[39mappend([cell\u001b[39m.\u001b[39mget_text()\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m row])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m table_text\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rows'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_table_text(pdf_path, page_number):\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    page = pdf_doc[page_number - 1]  # Pages are 0-indexed in fitz\n",
    "    \n",
    "    # Get the table blocks on the page\n",
    "    table_blocks = page.get_text_blocks()\n",
    "    print(table_blocks)\n",
    "    table_text = []\n",
    "    \n",
    "    for block in table_blocks:\n",
    "        for row in block.rows:\n",
    "            table_text.append([cell.get_text().strip() for cell in row])\n",
    "    \n",
    "    return table_text\n",
    "\n",
    "# Provide the path to your PDF file\n",
    "pdf_path = 'Document_Check_List_-_India (1).pdf'\n",
    "\n",
    "# Specify the page number containing the table\n",
    "page_number = 1\n",
    "\n",
    "table_text = extract_table_text(pdf_path, page_number)\n",
    "print(table_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Date :                                    Location :     ', 'Employee Name :     Claimant Name :    ', 'Claim intimation no :     Corporate Name :   ', 'Type of Claim (Main claim/Pre- Post/Deduction/Deficiency)    ', 'No of Pages submitted :  ', 'CHECK LIST  ', 'Sr. no  Particulars   Collected   Yes/No  ', '1  Duly Filled & signed Claim Form of IRDA.    ', '2  Original Discharge Card / Summary/Transfer Summary/Death  Summary    ', '3  Original Final Bill of the Hospital with breakup of all charges     ', '4  Original Bill Paid Receipt (Deposit/Final payment receipt) with  revenue stamp    ', '5  Original Investigation Reports (ECG, USG, CT Scan, X-ray, Blood  report, A scan etc)    ', '6  All Imaging Films, ECG Strips, Doppler / Angiogram CD etc    ', '7  Original Pharmacy bill with supporting prescriptions.     ', '8  Hospital Registration Certificate (in case of a unknown small  hospital)    ', '9  Any other original documents related to the claim.    ', '10  MLC/FIR in case of Accident cases.      ', '11  Copy of intimation mail       ', '12  Contact details of insured  & patient is mandatory    ', '13  Photo ID proof of patient is mandatory.       ', '14  Clarification letter if delay in submission of claim documents.      ', '15  KYC of patient and Insured, contact details of Claimant and  email  ', '16  Indoor case papers is mandatory.    ', '17  Aadhar Card of patient is mandatory. as per IRDA Circular  # IRDA/SDD/MISC/CIR/248/11/2017    ', '18  Original Cancel cheque with name of beneficiary & IFSC code    ', '  Name & Signature of Employee :   Name & signature of the receiver       ', '<image: DeviceRGB, width: 148, height: 42, bpc: 8>']\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_table_text(pdf_path, page_number):\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    page = pdf_doc[page_number - 1]  # Pages are 0-indexed in fitz\n",
    "    \n",
    "    # Get the table blocks on the page\n",
    "    table_blocks = page.get_text_blocks()\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    for block in table_blocks:\n",
    "        for row in block:\n",
    "            if isinstance(row, str):\n",
    "                row = row.replace('\\n', ' ')\n",
    "                if not row.isspace():\n",
    "                    text_list.append(row)\n",
    "            \n",
    "    return text_list\n",
    "\n",
    "# Provide the path to your PDF file\n",
    "pdf_path = 'Document_Check_List_-_India (1).pdf'\n",
    "\n",
    "# Specify the page number containing the table\n",
    "page_number = 1\n",
    "\n",
    "table_text = extract_table_text(pdf_path, page_number)\n",
    "print(table_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 28\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m token \u001b[39m=\u001b[39m nlp(word_or_sentence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Get the synonyms for the token\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m synonyms \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39;49mlemma_\u001b[39m.\u001b[39msynsets[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlemmas\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Print the synonyms\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(synonyms)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get the word or sentence to get synonyms for\n",
    "word_or_sentence = \"dog\"\n",
    "\n",
    "# Get the token object for the word or sentence\n",
    "token = nlp(word_or_sentence)\n",
    "\n",
    "# Get the synonyms for the token\n",
    "synonyms = token.lemma_.synsets[0].lemmas\n",
    "\n",
    "# Print the synonyms\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'destruct', 'demolish', 'ruin', 'destroy', 'put_down'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "#Creating a list \n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"destroy\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())#adding into synonyms\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.lexeme.Lexeme' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhappy birthday\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Enhance the search query with synonyms\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m enhanced_query \u001b[39m=\u001b[39m enhance_search(query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOriginal Query:\u001b[39m\u001b[39m\"\u001b[39m, query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnhanced Query:\u001b[39m\u001b[39m\"\u001b[39m, enhanced_query)\n",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Generate synonyms for nouns, verbs, adjectives, and adverbs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mpos_ \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mADJ\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mADV\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         synonyms \u001b[39m=\u001b[39m generate_synonyms(token\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         enhanced_query\u001b[39m.\u001b[39mextend(synonyms)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(enhanced_query)\n",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m token \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mvocab[word]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Retrieve similar words based on word vector similarity\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m lexeme \u001b[39min\u001b[39;00m token:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m lexeme\u001b[39m.\u001b[39mhas_vector \u001b[39mand\u001b[39;00m lexeme\u001b[39m.\u001b[39mtext \u001b[39m!=\u001b[39m word:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         synonyms\u001b[39m.\u001b[39mappend(lexeme\u001b[39m.\u001b[39mlemma_\u001b[39m.\u001b[39mname())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'spacy.lexeme.Lexeme' object is not iterable"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    synonyms = []\n",
    "\n",
    "    # Retrieve the word's token from the spaCy model\n",
    "    token = nlp.vocab[word]\n",
    "\n",
    "    # Retrieve similar words based on word vector similarity\n",
    "    for lexeme in token:\n",
    "        if lexeme.has_vector and lexeme.text != word:\n",
    "            synonyms.append(lexeme.lemma_.name())\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def enhance_search(query):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    enhanced_query = []\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokens = nlp(query)\n",
    "\n",
    "    for token in tokens:\n",
    "        # Add the original token\n",
    "        enhanced_query.append(token.text)\n",
    "\n",
    "        # Generate synonyms for nouns, verbs, adjectives, and adverbs\n",
    "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
    "            synonyms = generate_synonyms(token.text)\n",
    "            enhanced_query.extend(synonyms)\n",
    "\n",
    "    return \" \".join(enhanced_query)\n",
    "\n",
    "# User query\n",
    "query = \"happy birthday\"\n",
    "\n",
    "# Enhance the search query with synonyms\n",
    "enhanced_query = enhance_search(query)\n",
    "\n",
    "print(\"Original Query:\", query)\n",
    "print(\"Enhanced Query:\", enhanced_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cilvo/nltk_data'\n    - 'c:\\\\Users\\\\cilvo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\cilvo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\cilvo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cilvo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 32\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter a search query: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Enhance the search query\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m enhanced_query \u001b[39m=\u001b[39m enhance_search(query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Print the enhanced search query\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnhanced search query:\u001b[39m\u001b[39m\"\u001b[39m, enhanced_query)\n",
      "\u001b[1;32mc:\\Visual_Studio_Projects\\Neo4j_NLP\\graphNLP.ipynb Cell 32\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m enhanced_query \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Tokenize the query\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(query)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Get the synsets for each token\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Visual_Studio_Projects/Neo4j_NLP/graphNLP.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cilvo/nltk_data'\n    - 'c:\\\\Users\\\\cilvo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\cilvo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\cilvo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cilvo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def enhance_search(query):\n",
    "    enhanced_query = []\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokens = nltk.word_tokenize(query)\n",
    "\n",
    "    # Get the synsets for each token\n",
    "    for token in tokens:\n",
    "        synsets = nltk.wordnet.synsets(token)\n",
    "\n",
    "        # Get the lemmas for each synset\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                enhanced_query.append(lemma.name())\n",
    "\n",
    "    return \" \".join(enhanced_query)\n",
    "\n",
    "# User input\n",
    "query = input(\"Enter a search query: \")\n",
    "\n",
    "# Enhance the search query\n",
    "enhanced_query = enhance_search(query)\n",
    "\n",
    "# Print the enhanced search query\n",
    "print(\"Enhanced search query:\", enhanced_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  what is the main objective of computers?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(\"what's the main purpose of a computer?\", 25)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  What are the famous places we should not miss in Russia?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(\"list the places to visit in russia that we shouldn't miss?\", 46)\n",
      "('list some good places to visit in russia?', 40)\n",
      "('list some of the best places to visit in russia?', 38)\n",
      "('list some of the most amazing places we should not miss in russia?', 26)\n"
     ]
    }
   ],
   "source": [
    "from parrot import Parrot\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# uncomment to get reproducable paraphrase generations\n",
    "def random_state(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "random_state(1234)\n",
    "\n",
    "\n",
    "#Init models (make sure you init ONLY once if you integrate this to your code)\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n",
    "\n",
    "phrases = [\"what is the main objective of computers?\",\n",
    "           \"What are the famous places we should not miss in Russia?\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "  print(\"-\"*100)\n",
    "  print(\"Input_phrase: \", phrase)\n",
    "  print(\"-\"*100)\n",
    "  para_phrases = parrot.augment(input_phrase=phrase)\n",
    "  for para_phrase in para_phrases:\n",
    "   print(para_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recommend some places to visit in russia?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git\n",
      "  Cloning https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git to c:\\users\\cilvo\\appdata\\local\\temp\\pip-req-build-lt4h_jvv\n",
      "  Resolved https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git to commit 720a87a1ee557d8ed8d9a021adbdd1dd5616c5f9\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parrot==1.0) (4.28.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parrot==1.0) (0.1.99)\n",
      "Collecting python-Levenshtein (from parrot==1.0)\n",
      "  Downloading python_Levenshtein-0.21.0-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parrot==1.0) (2.2.2)\n",
      "Collecting fuzzywuzzy (from parrot==1.0)\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting Levenshtein==0.21.0 (from python-Levenshtein->parrot==1.0)\n",
      "  Downloading Levenshtein-0.21.0-cp310-cp310-win_amd64.whl (100 kB)\n",
      "     ------------------------------------ 100.9/100.9 kB 725.0 kB/s eta 0:00:00\n",
      "Collecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.0->python-Levenshtein->parrot==1.0)\n",
      "  Downloading rapidfuzz-3.0.0-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "     ---------------------------------------- 1.8/1.8 MB 825.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (0.15.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (1.9.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers->parrot==1.0) (0.13.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->parrot==1.0) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->parrot==1.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->parrot==1.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->parrot==1.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->parrot==1.0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->parrot==1.0) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers->parrot==1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers->parrot==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers->parrot==1.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers->parrot==1.0) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers->parrot==1.0) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers->parrot==1.0) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers->parrot==1.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->parrot==1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->parrot==1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->parrot==1.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->parrot==1.0) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers->parrot==1.0) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->sentence-transformers->parrot==1.0) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers->parrot==1.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers->parrot==1.0) (1.3.0)\n",
      "Building wheels for collected packages: parrot\n",
      "  Building wheel for parrot (setup.py): started\n",
      "  Building wheel for parrot (setup.py): finished with status 'done'\n",
      "  Created wheel for parrot: filename=parrot-1.0-py3-none-any.whl size=8631 sha256=a8a67817b7442f42a781f61fb545af7e8ad46b8d1b9b7762ca64ea350948bf2a\n",
      "  Stored in directory: C:\\Users\\cilvo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-gld36rtg\\wheels\\e8\\ee\\2a\\4d6a4b2a5c37f5f750e90fa79d2ad84f444fba9b050ecbbe6d\n",
      "Successfully built parrot\n",
      "Installing collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein, parrot\n",
      "  Attempting uninstall: parrot\n",
      "    Found existing installation: parrot 0.0.16\n",
      "    Uninstalling parrot-0.0.16:\n",
      "      Successfully uninstalled parrot-0.0.16\n",
      "Successfully installed Levenshtein-0.21.0 fuzzywuzzy-0.18.0 parrot-1.0 python-Levenshtein-0.21.0 rapidfuzz-3.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git 'C:\\Users\\cilvo\\AppData\\Local\\Temp\\pip-req-build-lt4h_jvv'\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cilvo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase 1: To apply for a half-day leave click on First Half or Second Half. For a full-day leave request choose Full- Day On the right-hand side the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval.\n",
      "Paraphrase 2: User can create Leave Request under Leave menu. Select the type of leave you wish to apply for. To apply for a half-day leave click on First Half or Second Half. For a full-day leave request choose Full- Day On the right-hand side the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval\n",
      "Paraphrase 3: User can Create Leave Request under Leave menu. Select the type of leave you wish to apply for. To apply for a half-day leave click on First Half or Second Half. For a full-day leave request choose Full- Day On the right-hand side the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval\n",
      "Paraphrase 4: To apply for a half-day leave click on First Half or Second Half. For a full-day leave request choose Full- Day On the right-hand side the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval\n",
      "Paraphrase 5: To apply for a half-day leave click on First Half or Second Half. For a full-day leave request choose Full-Day On the right-hand side the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")\n",
    "\n",
    "paragraph = '''User can Create Leave Request under Leave menu. Select the type of leave you wish to apply for. To apply for a half-day leave click on First Half or Second Half . For a full-day leave request choose Full- Day On right-hand side the user can view the holiday list as well as his/her leave balance as on date Fill up the details for the leave period and submit. This request goes to the reporting manager for approval. Post approval your leave will appear on your calendar. You can track the status in the Leave Menu under Leave Request PATH :- Home Page - Leave - Create Leave Request'''\n",
    "\n",
    "inputs = tokenizer.encode(\"paraphrase: \" + paragraph, return_tensors=\"pt\")\n",
    "\n",
    "paraphrases = model.generate(inputs, max_length=150, num_return_sequences=5, num_beams=5, temperature=1.0)\n",
    "\n",
    "for i, paraphrase in enumerate(paraphrases):\n",
    "    print(f\"Paraphrase {i + 1}: {tokenizer.decode(paraphrase, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "paraphrases_decoded =list(set([tokenizer.decode(paraphrase, skip_special_tokens=True) for paraphrase in paraphrases]))\n",
    "\n",
    "print (len(paraphrases_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 17:48:09  Samples:  157\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 1.003     CPU time: 0.031\n",
      "/   _/                      v4.4.0\n",
      "\n",
      "Program: c:\\Users\\cilvo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"db182ec2-703f-4fac-aaf6-e1809f3cf457\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\cilvo\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-47240lgEHmvG1c9q.json\n",
      "\n",
      "\u001b[31m0.997\u001b[0m ZMQInteractiveShell.run_ast_nodes\u001b[0m  \u001b[2mIPython\\core\\interactiveshell.py:3274\u001b[0m\n",
      "└─ \u001b[31m0.995\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<module>\u001b[0m  \u001b[2m..\\..\\..\\Temp\\ipykernel_18220\\412372190.py:1\u001b[0m\n",
      "   ├─ \u001b[33m0.464\u001b[0m DataFrame.__init__\u001b[0m  \u001b[2mpandas\\core\\frame.py:609\u001b[0m\n",
      "   │     [118 frames hidden]  \u001b[2mpandas, numpy, <built-in>, <__array_f...\u001b[0m\n",
      "   ├─ \u001b[33m0.331\u001b[0m DataFrame.__getattr__\u001b[0m  \u001b[2mpandas\\core\\generic.py:5888\u001b[0m\n",
      "   │     [44 frames hidden]  \u001b[2mpandas, <built-in>\u001b[0m\n",
      "   ├─ \u001b[32m0.103\u001b[0m Graph.__init__\u001b[0m  \u001b[2mpy2neo\\database.py:287\u001b[0m\n",
      "   │     [79 frames hidden]  \u001b[2mpy2neo, <built-in>, interchange, sock...\u001b[0m\n",
      "   ├─ \u001b[32m0.077\u001b[0m Graph.run\u001b[0m  \u001b[2mpy2neo\\database.py:395\u001b[0m\n",
      "   │     [39 frames hidden]  \u001b[2mpy2neo, <built-in>, interchange, uuid\u001b[0m\n",
      "   └─ \u001b[92m\u001b[2m0.017\u001b[0m [self]\u001b[0m  \u001b[2mNone\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyinstrument import Profiler\n",
    "\n",
    "profiler = Profiler()\n",
    "profiler.start()\n",
    "\n",
    "# Create a Graph object and connect to the database\n",
    "graph = Graph(\"bolt://localhost:7689\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Define a Cypher query to get all nodes with the given label and property value\n",
    "query = f\"MATCH (n)-[r:Topic]->(m) RETURN r.data AS headline, r.heading_list as list\"\n",
    "\n",
    "# Run the query and extract information\n",
    "results = graph.run(query)\n",
    "df = pd.DataFrame(results.data())\n",
    "topics = list(set(df.headline))\n",
    "topics_paras = list(df.list)\n",
    "# print(df)\n",
    "\n",
    "\n",
    "# code you want to profile\n",
    "\n",
    "profiler.stop()\n",
    "\n",
    "print(profiler.output_text(unicode=True, color=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reporting Manager To Approve Leave Request</td>\n",
       "      <td>[ Reporting Manager To Approve Leave Request]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shift Change Request Approval</td>\n",
       "      <td>[ Shift Change Request Approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reporting Manager Create Leave Request on beh...</td>\n",
       "      <td>[ Reporting Manager Create Leave Request on be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reporting Manager View Employee’s Leave Details</td>\n",
       "      <td>[ Reporting Manager View Employee’s Leave Deta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reporting Manager Create Employee’s Roster un...</td>\n",
       "      <td>[ Reporting Manager Create Employee’s Roster u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reporting Manager Upload Shift Roster</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3i HR Team For Any Issues / Queries</td>\n",
       "      <td>[ 3i HR Team For Any Issues / Queries]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Infotech Leave and Attendance Manual User Manu...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HonoHR Application URL: https://3i.honohr.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Leave and Attendance for Employee &amp; Reporting...</td>\n",
       "      <td>[ Leave and Attendance for Employee &amp; Reportin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Out on Duty Request - How to Track the Status...</td>\n",
       "      <td>[ Out on Duty Request - How to Track the Statu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Out on Duty Transactions PATH :- Home Page -&gt; ...</td>\n",
       "      <td>[Out on Duty Transactions PATH :- Home Page -&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Attendance Regularization Requests</td>\n",
       "      <td>[ Attendance Regularization Requests]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Attendance Regularization Transactions PATH :...</td>\n",
       "      <td>[ Attendance Regularization Transactions PATH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Attendance Calendar - Real Time Attendance Data</td>\n",
       "      <td>[ Attendance Calendar - Real Time Attendance D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shift Roster - How to View and Change Shift PATH</td>\n",
       "      <td>[ Shift Roster - How to View and Change Shift ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Attendance Calendar - Real Time Attendance Data</td>\n",
       "      <td>[ Attendance Calendar - Real Time Attendance D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Leave Request - How to Apply for a Leave Request</td>\n",
       "      <td>[leave request - how do i apply for a leave re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Leave Transactions PATH :- Home Page -&gt; Home P...</td>\n",
       "      <td>[Leave Transactions PATH :- Home Page -&gt; Home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Leave Balance PATH:- Home Page -&gt; Leave Balance</td>\n",
       "      <td>[ Leave Balance PATH:- Home Page -&gt; Leave Bala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Reporting Manager PATH :- Home Page</td>\n",
       "      <td>[ Reporting Manager PATH :- Home Page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Attendance Regularizations Approval (AR)</td>\n",
       "      <td>[ Attendance Regularizations Approval (AR)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Out on Duty Transactions (OD) - How to Approve...</td>\n",
       "      <td>[Out on Duty Transactions (OD) - How to Approv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  \\\n",
       "0          Reporting Manager To Approve Leave Request   \n",
       "1                       Shift Change Request Approval   \n",
       "2    Reporting Manager Create Leave Request on beh...   \n",
       "3     Reporting Manager View Employee’s Leave Details   \n",
       "4    Reporting Manager Create Employee’s Roster un...   \n",
       "5               Reporting Manager Upload Shift Roster   \n",
       "6                 3i HR Team For Any Issues / Queries   \n",
       "7   Infotech Leave and Attendance Manual User Manu...   \n",
       "8       HonoHR Application URL: https://3i.honohr.com   \n",
       "9    Leave and Attendance for Employee & Reporting...   \n",
       "10   Out on Duty Request - How to Track the Status...   \n",
       "11  Out on Duty Transactions PATH :- Home Page -> ...   \n",
       "12                 Attendance Regularization Requests   \n",
       "13   Attendance Regularization Transactions PATH :...   \n",
       "14    Attendance Calendar - Real Time Attendance Data   \n",
       "15   Shift Roster - How to View and Change Shift PATH   \n",
       "16    Attendance Calendar - Real Time Attendance Data   \n",
       "17   Leave Request - How to Apply for a Leave Request   \n",
       "18  Leave Transactions PATH :- Home Page -> Home P...   \n",
       "19    Leave Balance PATH:- Home Page -> Leave Balance   \n",
       "20                Reporting Manager PATH :- Home Page   \n",
       "21           Attendance Regularizations Approval (AR)   \n",
       "22  Out on Duty Transactions (OD) - How to Approve...   \n",
       "\n",
       "                                                 list  \n",
       "0       [ Reporting Manager To Approve Leave Request]  \n",
       "1                    [ Shift Change Request Approval]  \n",
       "2   [ Reporting Manager Create Leave Request on be...  \n",
       "3   [ Reporting Manager View Employee’s Leave Deta...  \n",
       "4   [ Reporting Manager Create Employee’s Roster u...  \n",
       "5                                                  []  \n",
       "6              [ 3i HR Team For Any Issues / Queries]  \n",
       "7                                                  []  \n",
       "8                                                  []  \n",
       "9   [ Leave and Attendance for Employee & Reportin...  \n",
       "10  [ Out on Duty Request - How to Track the Statu...  \n",
       "11  [Out on Duty Transactions PATH :- Home Page ->...  \n",
       "12              [ Attendance Regularization Requests]  \n",
       "13  [ Attendance Regularization Transactions PATH ...  \n",
       "14  [ Attendance Calendar - Real Time Attendance D...  \n",
       "15  [ Shift Roster - How to View and Change Shift ...  \n",
       "16  [ Attendance Calendar - Real Time Attendance D...  \n",
       "17  [leave request - how do i apply for a leave re...  \n",
       "18  [Leave Transactions PATH :- Home Page -> Home ...  \n",
       "19  [ Leave Balance PATH:- Home Page -> Leave Bala...  \n",
       "20             [ Reporting Manager PATH :- Home Page]  \n",
       "21        [ Attendance Regularizations Approval (AR)]  \n",
       "22  [Out on Duty Transactions (OD) - How to Approv...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ara = []\n",
    "len(ara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_list_maker_paraphrase(query_processed, sim_func):\n",
    "    '''\n",
    "    Calculates the similarity scores between the paraphrased queries and the topics using a specified similarity function.\n",
    "\n",
    "    Args:\n",
    "        query_processed (str): The processed query string.\n",
    "        sim_func (function): The similarity function to calculate the similarity scores.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of similarity scores between the paraphrased queries and topics.\n",
    "    '''\n",
    "    queries = paraphrase(query_processed)\n",
    "    sim_list = []\n",
    "    \n",
    "    topic_para_index = 0\n",
    "    while topic_para_index < len(topics_paras):\n",
    "        topic_para = topics_paras[topic_para_index]\n",
    "        topic_sim = []\n",
    "        \n",
    "        topic_index = 0\n",
    "        while topic_index < len(topic_para):\n",
    "            topic = topic_para[topic_index]\n",
    "            sim_query = []\n",
    "            \n",
    "            query_index = 0\n",
    "            while query_index < len(queries):\n",
    "                query = queries[query_index]\n",
    "                sim = sim_func(query, topic)\n",
    "                sim_query.append(sim)\n",
    "                query_index += 1\n",
    "            \n",
    "            avg_sim_query = sum(sim_query) / len(sim_query)\n",
    "            topic_sim.append(avg_sim_query)\n",
    "            topic_index += 1\n",
    "        \n",
    "        avg_topic = sum(topic_sim) / len(topic_sim)\n",
    "        sim_list.append(avg_topic)\n",
    "        topic_para_index += 1\n",
    "    \n",
    "    return sim_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_query = \"MATCH (pdf:PDF) RETURN pdf.name\"\n",
    "pdf_list = list (graph.run(cypher_query).to_table()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Group_Mediclaim_Policy_Infotech (1).pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
